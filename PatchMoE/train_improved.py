#!/usr/bin/env python3
"""
æ”¹è‰¯ã•ã‚ŒãŸPatchMoEå­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
- æ”¹è‰¯ã•ã‚ŒãŸæå¤±é–¢æ•°
- å¯¾ç…§å­¦ç¿’ã®çµ±åˆ
- ã‚ˆã‚Šå®‰å®šã—ãŸå­¦ç¿’
"""
import os
import json
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
import torch.optim as optim
from torch.utils.tensorboard import SummaryWriter
import numpy as np
from tqdm import tqdm
import argparse
from typing import Dict, Any

from PatchMoE.model import PatchMoEModel
from PatchMoE.medical_dataset import build_medical_loader
from PatchMoE.losses import PatchMoECombinedLoss
from PatchMoE.contrastive import AdvancedPatchContrastiveLoss, DomainAdaptiveContrastiveLoss


class ImprovedPatchMoETrainer:
    """æ”¹è‰¯ã•ã‚ŒãŸPatchMoEå­¦ç¿’ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, config: Dict[str, Any], device: str = 'cuda:0'):
        self.config = config
        self.device = device
        
        # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
        self.model = PatchMoEModel(
            in_ch=3,
            feat_dim=config['feat_dim'],
            grid_h=config['grid_h'],
            grid_w=config['grid_w'],
            num_datasets=config['num_datasets'],
            num_images=config['num_images_cap'],
            num_classes=config['num_classes'],
            num_layers=config['num_layers'],
            num_heads=config['num_heads'],
            num_queries=config['num_queries'],
            gate_top_k=config['top_k'],
            gate_capacity=config['capacity_factor'],
            gate_noise=config['gate_noise'],
            experts_per_device=config['experts_per_device'],
            backbone='resnet50',
            pretrained_backbone=True,
            use_multiscale=True
        ).to(device)
        
        # æå¤±é–¢æ•°
        self.criterion = PatchMoECombinedLoss(
            num_classes=config['num_classes'],
            dice_weight=1.0,
            focal_weight=1.0,
            contrastive_weight=0.1,
            moe_weight=0.01
        )
        
        # å¯¾ç…§å­¦ç¿’æå¤±
        self.contrastive_criterion = AdvancedPatchContrastiveLoss(
            temperature=0.07,
            hard_negative_weight=2.0,
            domain_separation_weight=1.5
        )
        
        # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼
        self.optimizer = optim.AdamW(
            self.model.parameters(),
            lr=config['lr'],
            weight_decay=config['weight_decay']
        )
        
        # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼
        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer, T_max=config['epochs'], eta_min=1e-6
        )
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
        self.train_loader = build_medical_loader(
            batch_size=config['batch_size'],
            length=1000,
            image_size=512,
            num_classes=config['num_classes'],
            num_datasets=config['num_datasets'],
            grid_h=config['grid_h'],
            grid_w=config['grid_w'],
            num_workers=0
        )
        
        # ãƒ­ã‚°è¨­å®š
        os.makedirs(config['out_dir'], exist_ok=True)
        self.writer = SummaryWriter(config['out_dir'])
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¿½è·¡
        self.best_dice = 0.0
        self.best_miou = 0.0
        
    def compute_metrics(self, logits: torch.Tensor, targets: torch.Tensor, 
                       mask_logits: torch.Tensor, mask_targets: torch.Tensor) -> Dict[str, float]:
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—"""
        # Diceä¿‚æ•°
        probs = torch.softmax(mask_logits, dim=1)
        pred = torch.argmax(probs, dim=1)
        
        # ãƒãƒ«ãƒã‚¯ãƒ©ã‚¹Dice
        dice_scores = []
        for c in range(mask_logits.size(1)):
            pred_c = (pred == c).float()
            target_c = (mask_targets == c).float()
            
            intersection = (pred_c * target_c).sum()
            union = pred_c.sum() + target_c.sum()
            dice = (2.0 * intersection + 1e-6) / (union + 1e-6)
            dice_scores.append(dice.item())
            
        dice = np.mean(dice_scores)
        
        # mIoU
        iou_scores = []
        for c in range(mask_logits.size(1)):
            pred_c = (pred == c).float()
            target_c = (mask_targets == c).float()
            
            intersection = (pred_c * target_c).sum()
            union = pred_c.sum() + target_c.sum() - intersection
            iou = intersection / (union + 1e-6)
            iou_scores.append(iou.item())
            
        miou = np.mean(iou_scores)
        
        return {'dice': dice, 'miou': miou}
    
    def train_epoch(self, epoch: int) -> Dict[str, float]:
        """1ã‚¨ãƒãƒƒã‚¯ã®å­¦ç¿’"""
        self.model.train()
        total_loss = 0.0
        total_metrics = {'dice': 0.0, 'miou': 0.0}
        num_batches = 0
        
        pbar = tqdm(self.train_loader, desc=f'Epoch {epoch}')
        for batch_idx, (images, dataset_ids, image_ids, masks, patch_classes) in enumerate(pbar):
            images = images.to(self.device)
            dataset_ids = dataset_ids.to(self.device)
            image_ids = image_ids.to(self.device)
            masks = masks.to(self.device)
            patch_classes = patch_classes.to(self.device)
            
            self.optimizer.zero_grad()
            
            # ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹
            logits, mask_logits = self.model(images, dataset_ids, image_ids)
            
            # å¯¾ç…§å­¦ç¿’ã®ãŸã‚ã®ç‰¹å¾´æŠ½å‡ºï¼ˆPPEã®å‡ºåŠ›ã‚’ä½¿ç”¨ï¼‰
            with torch.no_grad():
                # PPEç‰¹å¾´ã‚’å–å¾—ï¼ˆç°¡ç•¥åŒ–ï¼‰
                B, L, C = logits.shape
                patch_ids = torch.arange(L, device=self.device).unsqueeze(0).repeat(B, 1)
                
                # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆIDã¨ç”»åƒIDã‚’ãƒ‘ãƒƒãƒæ•°ã«åˆã‚ã›ã¦æ‹¡å¼µ
                dataset_ids_expanded = dataset_ids[:, 0:1].repeat(1, L)
                image_ids_expanded = image_ids[:, 0:1].repeat(1, L)
                
                coords = torch.stack([dataset_ids_expanded, image_ids_expanded, patch_ids], dim=-1)
                ppe_features = self.model.ppe(coords)
            
            # å¯¾ç…§å­¦ç¿’æå¤±
            contrastive_loss = self.contrastive_criterion(
                ppe_features, dataset_ids[:, 0:1].repeat(1, L), 
                image_ids[:, 0:1].repeat(1, L)
            )
            
            # MoEè² è·åˆ†æ•£æå¤±
            moe_aux_loss = torch.tensor(0.0, device=self.device)
            
            # ãƒã‚¹ã‚¯ã‚’é©åˆ‡ãªã‚µã‚¤ã‚ºã«ãƒªã‚µã‚¤ã‚º
            target_masks = F.interpolate(masks, size=mask_logits.shape[2:], mode='nearest').squeeze(1)
            
            # çµ±åˆæå¤±è¨ˆç®—
            loss_dict = self.criterion(
                logits, mask_logits, patch_classes, target_masks,
                contrastive_loss, moe_aux_loss
            )
            
            loss = loss_dict['total_loss']
            loss.backward()
            
            # ã‚°ãƒ©ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            
            self.optimizer.step()
            
            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
            with torch.no_grad():
                metrics = self.compute_metrics(logits, patch_classes, mask_logits, target_masks)
                total_metrics['dice'] += metrics['dice']
                total_metrics['miou'] += metrics['miou']
            
            total_loss += loss.item()
            num_batches += 1
            
            # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼æ›´æ–°
            pbar.set_postfix({
                'Loss': f'{loss.item():.4f}',
                'Dice': f'{metrics["dice"]:.3f}',
                'mIoU': f'{metrics["miou"]:.3f}',
                'Contrastive': f'{contrastive_loss.item():.4f}'
            })
            
            # ãƒ­ã‚°è¨˜éŒ²
            if batch_idx % self.config['log_interval'] == 0:
                global_step = epoch * len(self.train_loader) + batch_idx
                self.writer.add_scalar('Loss/Total', loss.item(), global_step)
                self.writer.add_scalar('Loss/Classification', loss_dict['cls_loss'].item(), global_step)
                self.writer.add_scalar('Loss/Segmentation', loss_dict['seg_loss'].item(), global_step)
                self.writer.add_scalar('Loss/Contrastive', contrastive_loss.item(), global_step)
                self.writer.add_scalar('Metrics/Dice', metrics['dice'], global_step)
                self.writer.add_scalar('Metrics/mIoU', metrics['miou'], global_step)
                self.writer.add_scalar('Learning_Rate', self.optimizer.param_groups[0]['lr'], global_step)
        
        # å¹³å‡ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        avg_metrics = {k: v / num_batches for k, v in total_metrics.items()}
        avg_loss = total_loss / num_batches
        
        return {'loss': avg_loss, **avg_metrics}
    
    def train(self):
        """å­¦ç¿’å®Ÿè¡Œ"""
        print(f"ğŸš€ æ”¹è‰¯ã•ã‚ŒãŸPatchMoEå­¦ç¿’ã‚’é–‹å§‹")
        print(f"ğŸ“Š è¨­å®š: {json.dumps(self.config, indent=2)}")
        
        for epoch in range(self.config['epochs']):
            # å­¦ç¿’
            train_metrics = self.train_epoch(epoch)
            
            # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼æ›´æ–°
            self.scheduler.step()
            
            # ã‚¨ãƒãƒƒã‚¯çµæœè¡¨ç¤º
            print(f"Epoch {epoch+1}/{self.config['epochs']}:")
            print(f"  Loss: {train_metrics['loss']:.4f}")
            print(f"  Dice: {train_metrics['dice']:.3f}")
            print(f"  mIoU: {train_metrics['miou']:.3f}")
            
            # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ä¿å­˜
            if train_metrics['dice'] > self.best_dice:
                self.best_dice = train_metrics['dice']
                self.best_miou = train_metrics['miou']
                
                # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜
                checkpoint = {
                    'epoch': epoch,
                    'model_state_dict': self.model.state_dict(),
                    'optimizer_state_dict': self.optimizer.state_dict(),
                    'scheduler_state_dict': self.scheduler.state_dict(),
                    'best_dice': self.best_dice,
                    'best_miou': self.best_miou,
                    'config': self.config
                }
                
                torch.save(checkpoint, os.path.join(self.config['out_dir'], 'best_model.pt'))
                
                # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ä¿å­˜
                with open(os.path.join(self.config['out_dir'], 'best_metrics.json'), 'w') as f:
                    json.dump({'dice': self.best_dice, 'miou': self.best_miou}, f, indent=2)
                
                print(f"  âœ… æ–°ã—ã„ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ä¿å­˜ (Dice: {self.best_dice:.3f})")
            
            print("-" * 50)
        
        print(f"ğŸ¯ å­¦ç¿’å®Œäº†! ãƒ™ã‚¹ãƒˆçµæœ:")
        print(f"  Dice: {self.best_dice:.3f}")
        print(f"  mIoU: {self.best_miou:.3f}")
        
        self.writer.close()


def main():
    parser = argparse.ArgumentParser(description='æ”¹è‰¯ã•ã‚ŒãŸPatchMoEå­¦ç¿’')
    parser.add_argument('--config', type=str, default=None, help='è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹')
    parser.add_argument('--output_dir', type=str, default='/workspace/outputs/patchmoe_improved', help='å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª')
    parser.add_argument('--epochs', type=int, default=10, help='ã‚¨ãƒãƒƒã‚¯æ•°')
    parser.add_argument('--batch_size', type=int, default=4, help='ãƒãƒƒãƒã‚µã‚¤ã‚º')
    parser.add_argument('--lr', type=float, default=1e-4, help='å­¦ç¿’ç‡')
    args = parser.parse_args()
    
    # è¨­å®š
    config = {
        'feat_dim': 128,
        'grid_h': 16,
        'grid_w': 16,
        'num_classes': 6,
        'num_datasets': 4,
        'num_images_cap': 100000,
        'num_queries': 25,
        'num_layers': 8,
        'num_heads': 8,
        'top_k': 2,
        'capacity_factor': 1.0,
        'gate_noise': 1.0,
        'experts_per_device': 4,
        'lr': args.lr,
        'weight_decay': 1e-2,
        'epochs': args.epochs,
        'batch_size': args.batch_size,
        'out_dir': args.output_dir,
        'log_interval': 10
    }
    
    # å­¦ç¿’å®Ÿè¡Œ
    trainer = ImprovedPatchMoETrainer(config)
    trainer.train()


if __name__ == '__main__':
    main()
