#!/usr/bin/env python3
"""
é«˜åº¦æŠ€è¡“ã‚’çµ±åˆã—ãŸPatchMoEå­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
- Paretoæœ€é©åŒ–
- ãƒ‰ãƒ¡ã‚¤ãƒ³é©å¿œ
- è»¢ç§»å­¦ç¿’
- ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’
"""
import os
import json
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
import torch.optim as optim
from torch.utils.tensorboard import SummaryWriter
import numpy as np
from tqdm import tqdm
import argparse
from typing import Dict, Any

from PatchMoE.advanced_techniques import (
    AdvancedPatchMoE, ParetoOptimizer, DomainAdaptationModule,
    create_advanced_patchmoe
)
from PatchMoE.medical_dataset import build_medical_loader
from PatchMoE.losses import PatchMoECombinedLoss
from PatchMoE.contrastive import AdvancedPatchContrastiveLoss


class AdvancedPatchMoETrainer:
    """é«˜åº¦æŠ€è¡“ã‚’çµ±åˆã—ãŸPatchMoEå­¦ç¿’ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, config: Dict[str, Any], device: str = 'cuda:0'):
        self.config = config
        self.device = device
        
        # é«˜åº¦æŠ€è¡“çµ±åˆãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ
        self.model = create_advanced_patchmoe(
            base_config=config,
            advanced_config=config.get('advanced', {})
        ).to(device)
        
        # æå¤±é–¢æ•°
        self.criterion = PatchMoECombinedLoss(
            num_classes=config['num_classes'],
            dice_weight=1.0,
            focal_weight=1.0,
            contrastive_weight=0.1,
            moe_weight=0.01
        )
        
        # å¯¾ç…§å­¦ç¿’æå¤±
        self.contrastive_criterion = AdvancedPatchContrastiveLoss(
            temperature=0.07,
            hard_negative_weight=2.0,
            domain_separation_weight=1.5
        )
        
        # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼
        self.optimizer = optim.AdamW(
            self.model.parameters(),
            lr=config['lr'],
            weight_decay=config['weight_decay']
        )
        
        # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼
        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer, T_max=config['epochs'], eta_min=1e-6
        )
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
        self.train_loader = build_medical_loader(
            batch_size=config['batch_size'],
            length=1000,
            image_size=512,
            num_classes=config['num_classes'],
            num_datasets=config['num_datasets'],
            grid_h=config['grid_h'],
            grid_w=config['grid_w'],
            num_workers=0
        )
        
        # ãƒ­ã‚°è¨­å®š
        os.makedirs(config['out_dir'], exist_ok=True)
        self.writer = SummaryWriter(config['out_dir'])
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¿½è·¡
        self.best_dice = 0.0
        self.best_miou = 0.0
        
        # Paretoæœ€é©åŒ–å™¨
        self.pareto_optimizer = ParetoOptimizer(num_objectives=4)
        
    def compute_metrics(self, logits: torch.Tensor, targets: torch.Tensor, 
                       mask_logits: torch.Tensor, mask_targets: torch.Tensor) -> Dict[str, float]:
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—"""
        # Diceä¿‚æ•°
        probs = torch.softmax(mask_logits, dim=1)
        pred = torch.argmax(probs, dim=1)
        
        # ãƒãƒ«ãƒã‚¯ãƒ©ã‚¹Dice
        dice_scores = []
        for c in range(mask_logits.size(1)):
            pred_c = (pred == c).float()
            target_c = (mask_targets == c).float()
            
            intersection = (pred_c * target_c).sum()
            union = pred_c.sum() + target_c.sum()
            dice = (2.0 * intersection + 1e-6) / (union + 1e-6)
            dice_scores.append(dice.item())
            
        dice = np.mean(dice_scores)
        
        # mIoU
        iou_scores = []
        for c in range(mask_logits.size(1)):
            pred_c = (pred == c).float()
            target_c = (mask_targets == c).float()
            
            intersection = (pred_c * target_c).sum()
            union = pred_c.sum() + target_c.sum() - intersection
            iou = intersection / (union + 1e-6)
            iou_scores.append(iou.item())
            
        miou = np.mean(iou_scores)
        
        return {'dice': dice, 'miou': miou}
    
    def train_epoch(self, epoch: int) -> Dict[str, float]:
        """1ã‚¨ãƒãƒƒã‚¯ã®å­¦ç¿’"""
        self.model.train()
        total_loss = 0.0
        total_metrics = {'dice': 0.0, 'miou': 0.0}
        num_batches = 0
        
        # Paretoæœ€é©åŒ–ã®ãŸã‚ã®æå¤±è¿½è·¡
        pareto_losses = []
        
        pbar = tqdm(self.train_loader, desc=f'Advanced Epoch {epoch}')
        for batch_idx, (images, dataset_ids, image_ids, masks, patch_classes) in enumerate(pbar):
            images = images.to(self.device)
            dataset_ids = dataset_ids.to(self.device)
            image_ids = image_ids.to(self.device)
            masks = masks.to(self.device)
            patch_classes = patch_classes.to(self.device)
            
            self.optimizer.zero_grad()
            
            # é«˜åº¦æŠ€è¡“çµ±åˆãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹
            logits, mask_logits, additional_losses = self.model(
                images, dataset_ids, image_ids, dataset_ids[:, 0:1]
            )
            
            # ãƒã‚¹ã‚¯ã‚’é©åˆ‡ãªã‚µã‚¤ã‚ºã«ãƒªã‚µã‚¤ã‚º
            target_masks = F.interpolate(masks, size=mask_logits.shape[2:], mode='nearest').squeeze(1)
            
            # å¯¾ç…§å­¦ç¿’ã®ãŸã‚ã®ç‰¹å¾´æŠ½å‡º
            with torch.no_grad():
                B, L, C = logits.shape
                patch_ids = torch.arange(L, device=self.device).unsqueeze(0).repeat(B, 1)
                dataset_ids_expanded = dataset_ids[:, 0:1].repeat(1, L)
                image_ids_expanded = image_ids[:, 0:1].repeat(1, L)
                coords = torch.stack([dataset_ids_expanded, image_ids_expanded, patch_ids], dim=-1)
                ppe_features = self.model.base_model.ppe(coords)
            
            # å¯¾ç…§å­¦ç¿’æå¤±
            contrastive_loss = self.contrastive_criterion(
                ppe_features, dataset_ids_expanded, image_ids_expanded
            )
            
            # çµ±åˆæå¤±è¨ˆç®—
            loss_dict = self.criterion(
                logits, mask_logits, patch_classes, target_masks,
                contrastive_loss, torch.tensor(0.0, device=self.device)
            )
            
            # é«˜åº¦æŠ€è¡“ã®æå¤±ã‚’è¿½åŠ 
            total_loss_tensor = loss_dict['total_loss']
            
            if 'domain_loss' in additional_losses:
                domain_loss = additional_losses['domain_loss']
                total_loss_tensor += 0.1 * domain_loss
                loss_dict['domain_loss'] = domain_loss
                
            if 'kd_loss' in additional_losses:
                kd_loss = additional_losses['kd_loss']
                total_loss_tensor += 0.1 * kd_loss
                loss_dict['kd_loss'] = kd_loss
                
            # Paretoæœ€é©åŒ–ã®ãŸã‚ã®æå¤±åé›†
            pareto_losses.append(torch.stack([
                loss_dict['cls_loss'],
                loss_dict['seg_loss'],
                contrastive_loss,
                additional_losses.get('domain_loss', torch.tensor(0.0, device=self.device))
            ]))
            
            # ãƒãƒƒã‚¯ãƒ—ãƒ­ãƒ‘ã‚²ãƒ¼ã‚·ãƒ§ãƒ³
            total_loss_tensor.backward()
            
            # ã‚°ãƒ©ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            
            self.optimizer.step()
            
            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
            with torch.no_grad():
                metrics = self.compute_metrics(logits, patch_classes, mask_logits, target_masks)
                total_metrics['dice'] += metrics['dice']
                total_metrics['miou'] += metrics['miou']
            
            total_loss += total_loss_tensor.item()
            num_batches += 1
            
            # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼æ›´æ–°
            pbar.set_postfix({
                'Loss': f'{total_loss_tensor.item():.4f}',
                'Dice': f'{metrics["dice"]:.3f}',
                'mIoU': f'{metrics["miou"]:.3f}',
                'Contrastive': f'{contrastive_loss.item():.4f}',
                'Domain': f'{additional_losses.get("domain_loss", torch.tensor(0.0)).item():.4f}'
            })
            
            # ãƒ­ã‚°è¨˜éŒ²
            if batch_idx % self.config['log_interval'] == 0:
                global_step = epoch * len(self.train_loader) + batch_idx
                self.writer.add_scalar('Loss/Total', total_loss_tensor.item(), global_step)
                self.writer.add_scalar('Loss/Classification', loss_dict['cls_loss'].item(), global_step)
                self.writer.add_scalar('Loss/Segmentation', loss_dict['seg_loss'].item(), global_step)
                self.writer.add_scalar('Loss/Contrastive', contrastive_loss.item(), global_step)
                
                if 'domain_loss' in additional_losses:
                    self.writer.add_scalar('Loss/Domain', additional_losses['domain_loss'].item(), global_step)
                if 'kd_loss' in additional_losses:
                    self.writer.add_scalar('Loss/KnowledgeDistillation', additional_losses['kd_loss'].item(), global_step)
                    
                self.writer.add_scalar('Metrics/Dice', metrics['dice'], global_step)
                self.writer.add_scalar('Metrics/mIoU', metrics['miou'], global_step)
                self.writer.add_scalar('Learning_Rate', self.optimizer.param_groups[0]['lr'], global_step)
        
        # Paretoæœ€é©åŒ–ã«ã‚ˆã‚‹é‡ã¿èª¿æ•´
        if len(pareto_losses) > 0:
            avg_pareto_losses = torch.stack(pareto_losses).mean(dim=0)
            optimized_weights = self.pareto_optimizer.update_weights(avg_pareto_losses)
            print(f"Paretoæœ€é©åŒ–é‡ã¿: {optimized_weights.cpu().numpy()}")
        
        # å¹³å‡ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        avg_metrics = {k: v / num_batches for k, v in total_metrics.items()}
        avg_loss = total_loss / num_batches
        
        return {'loss': avg_loss, **avg_metrics}
    
    def train(self):
        """å­¦ç¿’å®Ÿè¡Œ"""
        print(f"ğŸš€ é«˜åº¦æŠ€è¡“çµ±åˆPatchMoEå­¦ç¿’ã‚’é–‹å§‹")
        print(f"ğŸ“Š è¨­å®š: {json.dumps(self.config, indent=2)}")
        
        for epoch in range(self.config['epochs']):
            # å­¦ç¿’
            train_metrics = self.train_epoch(epoch)
            
            # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼æ›´æ–°
            self.scheduler.step()
            
            # ã‚¨ãƒãƒƒã‚¯çµæœè¡¨ç¤º
            print(f"Epoch {epoch+1}/{self.config['epochs']}:")
            print(f"  Loss: {train_metrics['loss']:.4f}")
            print(f"  Dice: {train_metrics['dice']:.3f}")
            print(f"  mIoU: {train_metrics['miou']:.3f}")
            
            # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ä¿å­˜
            if train_metrics['dice'] > self.best_dice:
                self.best_dice = train_metrics['dice']
                self.best_miou = train_metrics['miou']
                
                # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜
                checkpoint = {
                    'epoch': epoch,
                    'model_state_dict': self.model.state_dict(),
                    'optimizer_state_dict': self.optimizer.state_dict(),
                    'scheduler_state_dict': self.scheduler.state_dict(),
                    'best_dice': self.best_dice,
                    'best_miou': self.best_miou,
                    'config': self.config
                }
                
                torch.save(checkpoint, os.path.join(self.config['out_dir'], 'best_advanced_model.pt'))
                
                # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ä¿å­˜
                with open(os.path.join(self.config['out_dir'], 'best_advanced_metrics.json'), 'w') as f:
                    json.dump({'dice': self.best_dice, 'miou': self.best_miou}, f, indent=2)
                
                print(f"  âœ… æ–°ã—ã„ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ä¿å­˜ (Dice: {self.best_dice:.3f})")
            
            print("-" * 50)
        
        print(f"ğŸ¯ é«˜åº¦æŠ€è¡“çµ±åˆå­¦ç¿’å®Œäº†! ãƒ™ã‚¹ãƒˆçµæœ:")
        print(f"  Dice: {self.best_dice:.3f}")
        print(f"  mIoU: {self.best_miou:.3f}")
        
        self.writer.close()


def main():
    parser = argparse.ArgumentParser(description='é«˜åº¦æŠ€è¡“çµ±åˆPatchMoEå­¦ç¿’')
    parser.add_argument('--config', type=str, default=None, help='è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹')
    parser.add_argument('--output_dir', type=str, default='/workspace/outputs/patchmoe_advanced', help='å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª')
    parser.add_argument('--epochs', type=int, default=5, help='ã‚¨ãƒãƒƒã‚¯æ•°')
    parser.add_argument('--batch_size', type=int, default=2, help='ãƒãƒƒãƒã‚µã‚¤ã‚º')
    parser.add_argument('--lr', type=float, default=1e-4, help='å­¦ç¿’ç‡')
    parser.add_argument('--use_domain_adaptation', action='store_true', help='ãƒ‰ãƒ¡ã‚¤ãƒ³é©å¿œã‚’ä½¿ç”¨')
    parser.add_argument('--use_knowledge_distillation', action='store_true', help='çŸ¥è­˜è’¸ç•™ã‚’ä½¿ç”¨')
    args = parser.parse_args()
    
    # è¨­å®š
    config = {
        'in_ch': 3,
        'feat_dim': 128,
        'grid_h': 16,
        'grid_w': 16,
        'num_classes': 6,
        'num_datasets': 4,
        'num_images_cap': 100000,
        'num_queries': 25,
        'num_layers': 8,
        'num_heads': 8,
        'top_k': 2,
        'capacity_factor': 1.0,
        'gate_noise': 1.0,
        'experts_per_device': 4,
        'lr': args.lr,
        'weight_decay': 1e-2,
        'epochs': args.epochs,
        'batch_size': args.batch_size,
        'out_dir': args.output_dir,
        'log_interval': 10,
        'advanced': {
            'num_domains': 4,
            'use_domain_adaptation': args.use_domain_adaptation,
            'use_knowledge_distillation': args.use_knowledge_distillation
        }
    }
    
    # å­¦ç¿’å®Ÿè¡Œ
    trainer = AdvancedPatchMoETrainer(config)
    trainer.train()


if __name__ == '__main__':
    main()
